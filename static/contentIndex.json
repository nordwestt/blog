{"University/ChatGPT-Music-Player-plugin":{"title":"Rest Player (a ChatGPT plugin)","links":["static/openapi.yaml"],"tags":["ChatGPT","music"],"content":"\nRest Player\nA restful music player, allowing songs to be queued and played through HTTP calls.\n\nCan be used as a ChatGPT plugin\n\nDescription\nCurrently only compatible with Linux, this player makes use of the Python yt-dlp package to search for and stream songs from YouTube.\nIt has a simple queue, which can be added to, skipped through and cleared through the use of HTTP calls.\nPlease read the OpenAPI documentation to see how to integrate this into your own application.\nInstallation\npip install -r requirements.txt\nRunning (as a ChatGPT plugin)\nOpen a terminal and run:\n\nflask ‚Äîapp ytplay.py ‚Äîdebug run ‚Äîhost 0.0.0.0\n\nThen,\n\nopen up ChatGPT\nnavigate to the plugin store\nclick ‚ÄòDevelop your own plugin‚Äô in the bottom right\nin the field type in ‚Äòlocalhost:5000‚Äô and hit ‚ÄòFind manifest file‚Äô\n\nYou should see the plugin showing up as an option in ChatGPT now.\nUsing it (through ChatGPT)\nSimply tell chatgpt to put on a song of your liking or songs of a certain mood, get creative! You can also tell it to stop playback, skip songs or clear the queue out.\n\nExample prompts:\n\n\n‚ÄúI really like the steadfast beat of Stayin Alive. Please play some songs which have a similar solid beat to them‚Äù\n\n\n‚ÄúPlay some Michael Jackson for me‚Äù\n\n\n‚ÄúI love both Rock music and Middle Eastern music, can you play me some music that‚Äôs a fusion of the two?‚Äù\n\n\n‚ÄúI‚Äôm tired now. Please stop the music.‚Äù\n\n\n\nGo see the repository!"},"University/Genre-classification":{"title":"Genre Classification","links":[],"tags":["NeuralNetwork"],"content":"\nAt its very simplest, the GenrePrediction server is a webserver that accepts requests containing a .wav audio clip, and responds with a prediction of the genre of music contained within the clip in the form of JSON.\nHow it works\nIt works by first loading in the wav audio file, wherafter it extracts various features relating to the quality of the audio using the following function:\ndef extract_features(data):\n    features = []\n    for d in data:\n        y = d[0]\n        chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n        rmse = librosa.feature.rms(y=y)\n        spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n        spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n        zcr = librosa.feature.zero_crossing_rate(y)\n        mfcc = librosa.feature.mfcc(y=y, sr=sr)\n        features.append([np.mean(chroma_stft), np.mean(rmse), np.mean(spec_cent), np.mean(spec_bw), np.mean(rolloff), np.mean(zcr)]+[np.mean(mfcc[i]) for i in range(len(mfcc))])\n    return np.array(features)\n\n\nChroma_STFT: Chroma Short-Time Fourier Transform (STFT) represents the tonal content of the audio signal. It maps the audio signal into 12 distinct bins, representing the 12 musical notes in an octave. It can help differentiate genres based on their tonal characteristics and harmonic content.\n\n\nRMSE: Root Mean Square Energy (RMSE) is a measure of the audio signal‚Äôs energy or loudness. It can be useful for distinguishing genres with different overall energy levels, such as heavy metal (high energy) and ambient music (low energy).\n\n\nSpectral Centroid: Spectral Centroid is a measure of the center of mass of the frequency spectrum. It can be interpreted as the ‚Äúbrightness‚Äù of the sound. A higher spectral centroid value indicates a brighter sound, which might help differentiate genres like classical (bright) and electronic (darker).\n\n\nSpectral Bandwidth: Spectral Bandwidth represents the frequency spread of the audio signal. It can be useful for identifying genres with different frequency distributions, such as classical music (narrow bandwidth) and electronic music (wide bandwidth).\n\n\nSpectral Rolloff: Spectral Rolloff is the frequency below which a certain percentage (usually 85% or 95%) of the total energy of the spectrum is contained. It can help differentiate genres based on their high-frequency content, such as electronic music (high rolloff) and classical music (low rolloff).\n\n\nZero Crossing Rate (ZCR): ZCR is the rate at which the audio signal changes its sign (crosses zero). It can help identify genres with different amounts of high-frequency content, like rock music (high ZCR) and classical music (low ZCR).\n\n\nMel-Frequency Cepstral Coefficients (MFCCs): MFCCs are a set of features that describe the shape of the power spectrum of an audio signal. They are widely used in audio processing tasks, including music genre classification. MFCCs can capture timbral and spectral characteristics of different music genres, which can help the neural network distinguish between them.\n\n\nThese extracted features represent different aspects of the audio signal, such as tonality, energy, frequency distribution, and timbral characteristics.\nBy providing these features to the neural network, it‚Äôs able to differentiate music genres based on their unique sonic properties and output ten values, each representing the certainty of the audio being associated with a genre.\nThe output is then converted into a JSON format in order to be easily consumable by any application as a web-service.\nHow it was made\nTo train the neural network, the GTZAN dataset was used.\nThis dataset contains 1000 30-second audio clips, which were split into 3 second audio clips, resulting in 10000 3-second audio clips.\nUsing the feature extraction mentioned above, training data was prepared, containing the features of the audio and the corresponding label.\nThen, the following neural network model was assembled:\nmodel = models.Sequential()\nmodel.add(layers.Dense(512, activation=&#039;relu&#039;, input_shape=(X_train.shape[1],)))\nmodel.add(layers.Dropout(0.3))\nmodel.add(layers.Dense(256, activation=&#039;relu&#039;))\nmodel.add(layers.Dropout(0.3))\nmodel.add(layers.Dense(128, activation=&#039;relu&#039;))\nmodel.add(layers.Dropout(0.2))\nmodel.add(layers.Dense(64, activation=&#039;relu&#039;))\nmodel.add(layers.Dropout(0.1))\nmodel.add(layers.Dense(10, activation=&#039;softmax&#039;))\nmodel.compile(optimizer=&#039;adam&#039;, loss=&#039;sparse_categorical_crossentropy&#039;, metrics=[&#039;accuracy&#039;])\nThe model features a tapered architecture, in which both the width of the neural layers, as well as the dropout degree, decreases. Dropout layers randomly set input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting.\nTo train the final model, only 4000 out of the 10000 clips were used as it that seemed to be the most appropriate amount for the size of this network, in terms of validation accuracy.\nThe resulting model has a validation accuracy of 83% and classifies audio into one of the following 10 genres:\n\n‚Äòrock‚Äô,‚Äòreggae‚Äô,‚Äòpop‚Äô,‚Äòmetal‚Äô,‚Äòjazz‚Äô,‚Äòhiphop‚Äô,‚Äòdisco‚Äô,‚Äòcountry‚Äô,‚Äòclassical‚Äô,‚Äòblues‚Äô"},"University/Open-day-badge":{"title":"Open Day Badge","links":[],"tags":["NeuralNetwork"],"content":"\nSmart Badge for University Open Day\nYoung people usually visit universities to find scout the places and see what their potential future university life might look like.\nFor this reason, presenting them with work actually produced as a result of the degree could serve as a really helpful display of what they can expect to work with and produce.\nThe Vision üåü\nOur team prototyped an open day badge for prospective students. The badge aims to be engaging and useful, whilst demonstrating the skills acquired throughout our degree.\nEquipped with an e-ink screen, microcontroller and batteries, this wearable badge connects with a custom-designed app to allow users to draw, write and upload pictures to it to show off.\nHow Does It Work? ü§î\nThe mobile app converts the user‚Äôs creation (text, drawing or picture) into a bitmap, which is transmitted to the badge over bluetooth.\nThe badge then communicates with the e-ink display to make the creation appear on it.\nWhy This Solution? üöÄ\nThrough iterative development, we team arrived at a functional prototype which met the minimum specifications laid out in the first semester. Lastly, extra functionality was added, to improve the appeal of the product.\nThe description given was purposefully open to interpretation, allowing freedom of choice\nand the team‚Äôs creative input, with the primary project requirement that an open day\nbadge be produced by the end of the project. The driving project requirements can be\nsummarised here:\n\nManufacturing cost of &lt;¬£10 per unit.\nMust be useful, reusable and engaging.\nMust be a wearable badge.\nShould preferably have a social aspect to it.\nShould showcase engineering ingenuity and attract prospective engineering candi-\ndates.\n\nDesign and planning\nBrainstorming\nThrough discussion, the team agreed to proceed with the idea of a battery-powered display; to which, users could print their name (or other details such as email, role, title etc) and project their own drawings. The customisability of the device fulfills the project requirements of being entertaining, fun and useful, as the user can continue to operate the device in the future, or adapt the hardware for personal projects.\nMultiple open day visitors can wear the badges and customise the displays to showcase\ninteresting drawings, fun quotes about themselves, jokes or any other text that can enhance the open day visit and act as an ‚Äôice-breaker‚Äô. This aspect of the device fulfills the social feature of the project.\nDevice Functionality and Research\nTo fulfill the functional requirements of a badge that can showcase text and drawings implies the requirement that the device includes a display that is user programmable or responsive to user inputs.\nAs the device shall be in a badge form-factor, the inclusion of wires during use would greatly impede usability. Therefore, means to facilitate user input are also required to interface with the badge (e.g. buttons with menu interface).\nTo enhance user experience, it was determined that it is preferable for the user to operate the\ndevice wirelessly and update the display in real-time. Doing so allows the user to wear the\nbadge on their shirt while operating it, which eliminates the need for the badge to be hand-\nheld and controlled via physical buttons before being worn again. The inclusion of buttons\nwould potentially impede the operational workflow of the device, as it is important for the\nuser to be welcomed by a seam-less and straightforward user experience, with a simplistic\nset of commands and features. Updating the display on-command allows for greater creative\ncontrol and further potential applications for the badge, which would be otherwise restricted\nif the badge was operated purely on-board with buttons and (for example) pre-stored images\nand text or an on-board, button-controlled user interface.\nTo complete the communication link between the badge and the user, the controlling node\nwas decided to be the user‚Äôs smartphone. Considering that the vast majority of likely users\nwill be teenage students or older, it is safe to assume that every potential user will own a\nsmartphone capable of connecting wirelessly to the badge. As smartphones pack processing\npower capable of much more demanding tasks, all tasks related to connection and control\nof the badge, as well as providing the user interface for text input and drawing, would be\noffloaded to the smartphone device.\nTherefore, the badge core functionality is limited to establishing a connection with the\nsmartphone, receiving data wirelessly and displaying the incoming image (which may show-\ncase text, drawings or other) to a display.\nOther functional requirements of the badge include an onboard battery to allow the product\nto operate as a standalone unit. A micro-controller is also included, in order to act as the\nreceiving node of the wireless connection and as an operator of the on-board display."},"University/Sonic-AR-navigation-for-cyclists":{"title":"AR navigation for cyclists","links":["assets/papers/Mobile_HCI_Coursework.pdf"],"tags":[],"content":"\nAugmented Reality for Cyclists: A New Navigation Experience üö¥‚Äç‚ôÇÔ∏è\nAs a cyclist, one is much more exposed than other modes of transport. For this reason, navigating busy streets while keeping an eye on the road can be dangerous challenge. Enter the world of Augmented Reality (AR) to improve the way cyclists navigate.\nThe Vision üåü\nOur team explored the application of AR navigation for cyclists. This application aims to provide both audio and visual cues to help cyclists navigate without diverting their attention from the road. Designed for smartphones and AR glasses, this app promises an intuitive and helpful experience.\nNote: The primary goal is to ensure that users can focus on the road while seamlessly receiving directions.\n\nHow Does It Work? ü§î\nThe application leverages the camera and GPS functionalities of smartphones. It overlays waypoints onto the real world, guiding the cyclist through their journey. Imagine having a virtual waypoint illuminated right in front of you, ensuring you never miss a turn and never need to look away from the road!\nAddressing Potential Distractions\nThe team is acutely aware of the potential distractions that visual and auditory feedback can cause. Hence, they‚Äôve chosen specific interaction concepts to mitigate these concerns:\n\n\nBinaural Peripheral Audio Cues: These are 3D sound cues that help in determining the correct direction by panning the audio between the left and right ear. The sound is a low-passed periodic wave, meaning its presence aims to minimize interference with the existing environmental sounds.\n\n\nAnimated Visual Waypoints: Rather than informing the user through occluding their field of view with text or by lighting up the entire path, the destination is visible as a waypoint which periodically fades in and out, allowing the full environment to be viewed.\n\n\nWhy This Solution?\nWe believed that the AR solution offered a unique and innovative approach to enhance the cycling experience. Not only does it promise to improve navigation, but it also aimed to elevate the safety of cyclists on the road.\nIn conclusion, we believe the future of cycling navigation could look promising with the integration of AR technology and encourage anyone to draw inspiration from our investigation. Please feel free to read the paper for more information."},"index":{"title":"Welcome to a corner of the internet","links":["University/Genre-classification","University/ChatGPT-Music-Player-plugin","University/Open-day-badge","University/Sonic-AR-navigation-for-cyclists"],"tags":[],"content":"Breathe.. have a glass of water.. and take a look around, see if there‚Äôs anything of interest\nThere‚Äôs not much here at the moment, but I‚Äôm always working on something in my sparetime and I plan to write freely about it for those curious enough to read.\nSo far I‚Äôve added a few posts about stuff I did at uni - hover over to get a preview!\nAI\n\nGenre classification\nChatGPT Music Player plugin\n\nHardware\n\nOpen day badge\n\nAugmented Reality\n\nSonic AR-navigation for cyclists\n"}}