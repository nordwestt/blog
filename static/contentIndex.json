{"Home-Assistant/Kokoro-wyoming-text-to-speech":{"title":"Kokoro wyoming text-to-speech","links":["Side-projects/Compass"],"tags":[],"content":"I‚Äôve fallen into the habit of staring at the ‚Äòmodels‚Äô page of huggingface like a hawk scanning the landscape for potential prey.\nOne day, I came across one of the lightest TTS models I‚Äôve seen called Kokoro-82M. I was astounded by the fidelity of the voices and immediately had to clone the project and get it running locally.\nTo my surprise, it ran smoothly and the voices were just as good as advertised. I was working on my other project Compass at the time, which aims to be a fully local AI assistant, and thought Kokoro would be perfect for it. However, around the same time, I had just received my Home Assistant Voice and found that the voices available for it were sub-par for living in 2025.\nSo, I decided that it would be more worthwhile to develop a TTS server that could work with Home Assistant and be used by everyone who knows how to spin up a docker container.\nThe existing TTS solution for HASS (Home Assistant) is Piper and fortunately it uses an open protocol called the Wyoming Protocol.\nSo the plan was simple:\n\nCreate a dummy ‚Äúhello world‚Äù project that sends a pre-recorded sound over Wyoming Protocol to HASS when receiving a TTS request\nImplement the Kokoro generator to generate speech from the TTS request\nEnsure it works well (this one‚Äôs the hardest!)\n\nHello world\nThere‚Äôs a Python library for wyoming which unfortunately it‚Äôs rather lacking in documentation. The Github page lays out the overall structure of the protocol, but doesn‚Äôt go into much detail about the library either.\nSo, I decided to clone the Piper project and slowly build up a separate project that contained the bare-bones of what was necessary, since the Piper project has fancy multi-threading and is split neatly into separate files with interesting scripts to run different parts.\nAfter a few nights of tinkering, I finally got it working. Importantly, I had to make sure I warmed everything up before the first request - otherwise it would simply fail - which I guess is due to some timeout issues.\nKokoro\nI implemented Kokoro using the official Kokoro library, which was very straightforward.\nI ran it on a local machine and it worked - however it was rather slow ü§î\nSentence splitting\nYep - it‚Äôs not enough to just pass the entire text to the TTS model. It needs to be split into sentences first.\nUsing regex, we can split the text into sentences based on punctuation marks, with the code pattern:\n    pattern = r&#039;(?&lt;=[.!?])\\s+&#039;\n    sentences = re.split(pattern, text)\n\nIt looks for any of these punctuation marks:\n\n\nPeriod (.)\nExclamation mark (!)\nQuestion mark (?)\n\n\nAfter finding one of these marks, it looks for one or more spaces.\n\nSo for example, if you had this text:\n&quot;It was a dark and stormy night. Do you remember the old days? The rain fell in torrents.&quot;\nThe regex would find the punctuation marks and split the text into the following sentences:\n[&quot;It was a dark and stormy night.&quot;, &quot;Do you remember the old days?&quot;, &quot;The rain fell in torrents.&quot;]\nWith this additional step, the TTS model can generate each sentence individually and hences start sending the audio chunks over after having processed only the first sentence.Much better!\nDocker\nI then decided to package it up into a Docker container and release it on Docker Hub, however I found that the image was taking up 10GB of space. This was due to the Kokoro library having a dependency on PyTorch and other heavy libraries!\nSo, back to the drawing board.\nKokoro-ONNX\nI found there was an alternative implementation of Kokoro called Kokoro-Onnx which uses ONNX to run the model. This was a much lighter image and worked great, while still being fast.\nFinally, I was able to release the Docker image, taking up only 1.29 GB of space! Much better :)\nI now have a fully local TTS solution that works well with Home Assistant, and is much higher quality than the default Piper voices. Feel free to check it out on Github and give me a star if you like it!"},"Side-projects/Compass":{"title":"Compass","links":[],"tags":[],"content":"\n ¬†¬†¬†¬†\n\n\nWhat is it?\nCompass is a modern, open-source Large Language Model (LLM) client designed to provide a seamless AI chat experience across multiple platforms. Built with React Native and Expo, it offers a rich set of features while maintaining high performance and user experience. The goal is to promote private, decentralized AI - whilst also maintaining access to cloud-based AI through API‚Äôs for those who need or prefer it.\nCheck out the live demo!\nDownload the Android app!\nHow does it work?\nThe application is a ‚Äúshell‚Äù that stores your conversations, providers and settings. It then connects a backend, which is responsible for the actual AI inference.\nThe backend can be any of the popular LLM providers, but preferably one that supports running locally such as Ollama or LlamaCpp.\nThe local models offer a lot of benefits, such as:\n\nAnonymity\nDiversity of thought\nCustomization, ie. you can entirely control the system prompt\nRemoval of vendor lock-in\nUsing models that have less bias and censorship\n\nI built the app using React Native and Expo, with Nativewind for styling.\nThis choice was made on two grounds:\n\nReact Native can run efficiently on both iOS and Android, and Expo provides a way to build the app for both platforms using a single codebase.\nSince the app is built with React Native, which uses JavaScript, it can be easily ported to other platforms (such as what I‚Äôve done for Linux using Tauri) and also benefits from the large ecosystem of JavaScript libraries.\n\nMy vision for the app\nI‚Äôm not an AI evangelist, but I do believe that it is becoming very useful for many people and will only continue to do so even more. Given that, I think it‚Äôs important for everyone‚Äôs sake that it is accessible and that it can be used privately (I might make a blog post about this in the future, but I‚Äôll save it for then.)\nSo, I ultimately want to make a product that is:\n\nPrivate by default\nDecentralised\nOpen source\nMulti-platform\nHarnesses the power of AI to make people more productive and happy\n\nI‚Äôm not sure how much I‚Äôll be able to achieve towards this goal with Compass, but it‚Äôs a good starting point :)\nPlease do check out the repository and give me some feedback, and maybe a start if you like the project!"},"Side-projects/Radiooooo-download-button":{"title":"Radiooooo download button","links":[],"tags":[],"content":"\nI‚Äôve discovered a lot of gems on Radiooooo and have many times wished I could save them.\nThis Chrome extension allows for exactly that by adding a download button next to the ‚Äòplay‚Äô and ‚Äòskip‚Äô buttons.\nHow does it work?\nA simple piece of JavaScript does all of the work, and it‚Äôs rather simple:\n1 . We search for the audio element that‚Äôs playing the current song - it must know the url of the source in order to play it\nlet url = document.getElementsByClassName(&quot;audio&quot;)[0].currentSrc;\n2 . We create a new resource (‚Äúa‚Äù) element and assign the url of the song to the element\nlet element = document.createElement(&#039;a&#039;);\nelement.href = url;\n3 . A button element is created and injected into the UI next to the ‚Äòplay‚Äô and ‚Äòskip‚Äô buttons\nlet downloadBtn = document.createElement(&#039;button&#039;);\n \nlet panel = document.getElementsByClassName(&quot;group command&quot;)[0];\npanel.appendChild(downloadBtn);\n4 . We wire the button up to trigger the fetching of the resource\ndownloadBtn.onclick = downloadSong; // the function that contains the above functionality\nAnd voila! A convenient button for downloading music\nVisit the repository"},"University/ChatGPT-Music-Player-plugin":{"title":"Rest Player (a ChatGPT plugin)","links":["static/openapi.yaml"],"tags":["ChatGPT","music"],"content":"\nRest Player\nA restful music player, allowing songs to be queued and played through HTTP calls.\n\nCan be used as a ChatGPT plugin\n\nDescription\nCurrently only compatible with Linux, this player makes use of the Python yt-dlp package to search for and stream songs from YouTube.\nIt has a simple queue, which can be added to, skipped through and cleared through the use of HTTP calls.\nPlease read the OpenAPI documentation to see how to integrate this into your own application.\nInstallation\npip install -r requirements.txt\nRunning (as a ChatGPT plugin)\nOpen a terminal and run:\n\nflask ‚Äîapp ytplay.py ‚Äîdebug run ‚Äîhost 0.0.0.0\n\nThen,\n\nopen up ChatGPT\nnavigate to the plugin store\nclick ‚ÄòDevelop your own plugin‚Äô in the bottom right\nin the field type in ‚Äòlocalhost:5000‚Äô and hit ‚ÄòFind manifest file‚Äô\n\nYou should see the plugin showing up as an option in ChatGPT now.\nUsing it (through ChatGPT)\nSimply tell chatgpt to put on a song of your liking or songs of a certain mood, get creative! You can also tell it to stop playback, skip songs or clear the queue out.\n\nExample prompts:\n\n\n‚ÄúI really like the steadfast beat of Stayin Alive. Please play some songs which have a similar solid beat to them‚Äù\n\n\n‚ÄúPlay some Michael Jackson for me‚Äù\n\n\n‚ÄúI love both Rock music and Middle Eastern music, can you play me some music that‚Äôs a fusion of the two?‚Äù\n\n\n‚ÄúI‚Äôm tired now. Please stop the music.‚Äù\n\n\n\nGo see the repository!"},"University/Genre-classification":{"title":"Genre Classification","links":[],"tags":["NeuralNetwork"],"content":"\nAt its very simplest, the GenrePrediction server is a webserver that accepts requests containing a .wav audio clip, and responds with a prediction of the genre of music contained within the clip in the form of JSON.\nHow it works\nIt works by first loading in the wav audio file, wherafter it extracts various features relating to the quality of the audio using the following function:\ndef extract_features(data):\n    features = []\n    for d in data:\n        y = d[0]\n        chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n        rmse = librosa.feature.rms(y=y)\n        spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n        spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n        zcr = librosa.feature.zero_crossing_rate(y)\n        mfcc = librosa.feature.mfcc(y=y, sr=sr)\n        features.append([np.mean(chroma_stft), np.mean(rmse), np.mean(spec_cent), np.mean(spec_bw), np.mean(rolloff), np.mean(zcr)]+[np.mean(mfcc[i]) for i in range(len(mfcc))])\n    return np.array(features)\n\n\nChroma_STFT: Chroma Short-Time Fourier Transform (STFT) represents the tonal content of the audio signal. It maps the audio signal into 12 distinct bins, representing the 12 musical notes in an octave. It can help differentiate genres based on their tonal characteristics and harmonic content.\n\n\nRMSE: Root Mean Square Energy (RMSE) is a measure of the audio signal‚Äôs energy or loudness. It can be useful for distinguishing genres with different overall energy levels, such as heavy metal (high energy) and ambient music (low energy).\n\n\nSpectral Centroid: Spectral Centroid is a measure of the center of mass of the frequency spectrum. It can be interpreted as the ‚Äúbrightness‚Äù of the sound. A higher spectral centroid value indicates a brighter sound, which might help differentiate genres like classical (bright) and electronic (darker).\n\n\nSpectral Bandwidth: Spectral Bandwidth represents the frequency spread of the audio signal. It can be useful for identifying genres with different frequency distributions, such as classical music (narrow bandwidth) and electronic music (wide bandwidth).\n\n\nSpectral Rolloff: Spectral Rolloff is the frequency below which a certain percentage (usually 85% or 95%) of the total energy of the spectrum is contained. It can help differentiate genres based on their high-frequency content, such as electronic music (high rolloff) and classical music (low rolloff).\n\n\nZero Crossing Rate (ZCR): ZCR is the rate at which the audio signal changes its sign (crosses zero). It can help identify genres with different amounts of high-frequency content, like rock music (high ZCR) and classical music (low ZCR).\n\n\nMel-Frequency Cepstral Coefficients (MFCCs): MFCCs are a set of features that describe the shape of the power spectrum of an audio signal. They are widely used in audio processing tasks, including music genre classification. MFCCs can capture timbral and spectral characteristics of different music genres, which can help the neural network distinguish between them.\n\n\nThese extracted features represent different aspects of the audio signal, such as tonality, energy, frequency distribution, and timbral characteristics.\nBy providing these features to the neural network, it‚Äôs able to differentiate music genres based on their unique sonic properties and output ten values, each representing the certainty of the audio being associated with a genre.\nThe output is then converted into a JSON format in order to be easily consumable by any application as a web-service.\nHow it was made\nTo train the neural network, the GTZAN dataset was used.\nThis dataset contains 1000 30-second audio clips, which were split into 3 second audio clips, resulting in 10000 3-second audio clips.\nUsing the feature extraction mentioned above, training data was prepared, containing the features of the audio and the corresponding label.\nThen, the following neural network model was assembled:\nmodel = models.Sequential()\nmodel.add(layers.Dense(512, activation=&#039;relu&#039;, input_shape=(X_train.shape[1],)))\nmodel.add(layers.Dropout(0.3))\nmodel.add(layers.Dense(256, activation=&#039;relu&#039;))\nmodel.add(layers.Dropout(0.3))\nmodel.add(layers.Dense(128, activation=&#039;relu&#039;))\nmodel.add(layers.Dropout(0.2))\nmodel.add(layers.Dense(64, activation=&#039;relu&#039;))\nmodel.add(layers.Dropout(0.1))\nmodel.add(layers.Dense(10, activation=&#039;softmax&#039;))\nmodel.compile(optimizer=&#039;adam&#039;, loss=&#039;sparse_categorical_crossentropy&#039;, metrics=[&#039;accuracy&#039;])\nThe model features a tapered architecture, in which both the width of the neural layers, as well as the dropout degree, decreases. Dropout layers randomly set input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting.\nTo train the final model, only 4000 out of the 10000 clips were used as it that seemed to be the most appropriate amount for the size of this network, in terms of validation accuracy.\nThe resulting model has a validation accuracy of 83% and classifies audio into one of the following 10 genres:\n\n‚Äòrock‚Äô,‚Äòreggae‚Äô,‚Äòpop‚Äô,‚Äòmetal‚Äô,‚Äòjazz‚Äô,‚Äòhiphop‚Äô,‚Äòdisco‚Äô,‚Äòcountry‚Äô,‚Äòclassical‚Äô,‚Äòblues‚Äô"},"University/Open-day-badge":{"title":"Open Day Badge","links":[],"tags":["NeuralNetwork"],"content":"\nSmart Badge for University Open Day\nYoung people usually visit universities to find scout the places and see what their potential future university life might look like.\nFor this reason, presenting them with work actually produced as a result of the degree could serve as a really helpful display of what they can expect to work with and produce.\nThe Vision üåü\nOur team prototyped an open day badge for prospective students. The badge aims to be engaging and useful, whilst demonstrating the skills acquired throughout our degree.\nEquipped with an e-ink screen, microcontroller and batteries, this wearable badge connects with a custom-designed app to allow users to draw, write and upload pictures to it to show off.\nHow Does It Work? ü§î\nThe mobile app converts the user‚Äôs creation (text, drawing or picture) into a bitmap, which is transmitted to the badge over bluetooth.\nThe badge then communicates with the e-ink display to make the creation appear on it.\nWhy This Solution? üöÄ\nThrough iterative development, we team arrived at a functional prototype which met the minimum specifications laid out in the first semester. Lastly, extra functionality was added, to improve the appeal of the product.\nThe description given was purposefully open to interpretation, allowing freedom of choice\nand the team‚Äôs creative input, with the primary project requirement that an open day\nbadge be produced by the end of the project. The driving project requirements can be\nsummarised here:\n\nManufacturing cost of &lt;¬£10 per unit.\nMust be useful, reusable and engaging.\nMust be a wearable badge.\nShould preferably have a social aspect to it.\nShould showcase engineering ingenuity and attract prospective engineering candi-\ndates.\n\nDesign and planning\nBrainstorming\nThrough discussion, the team agreed to proceed with the idea of a battery-powered display; to which, users could print their name (or other details such as email, role, title etc) and project their own drawings. The customisability of the device fulfills the project requirements of being entertaining, fun and useful, as the user can continue to operate the device in the future, or adapt the hardware for personal projects.\nMultiple open day visitors can wear the badges and customise the displays to showcase\ninteresting drawings, fun quotes about themselves, jokes or any other text that can enhance the open day visit and act as an ‚Äôice-breaker‚Äô. This aspect of the device fulfills the social feature of the project.\nDevice Functionality and Research\nTo fulfill the functional requirements of a badge that can showcase text and drawings implies the requirement that the device includes a display that is user programmable or responsive to user inputs.\nAs the device shall be in a badge form-factor, the inclusion of wires during use would greatly impede usability. Therefore, means to facilitate user input are also required to interface with the badge (e.g. buttons with menu interface).\nTo enhance user experience, it was determined that it is preferable for the user to operate the\ndevice wirelessly and update the display in real-time. Doing so allows the user to wear the\nbadge on their shirt while operating it, which eliminates the need for the badge to be hand-\nheld and controlled via physical buttons before being worn again. The inclusion of buttons\nwould potentially impede the operational workflow of the device, as it is important for the\nuser to be welcomed by a seam-less and straightforward user experience, with a simplistic\nset of commands and features. Updating the display on-command allows for greater creative\ncontrol and further potential applications for the badge, which would be otherwise restricted\nif the badge was operated purely on-board with buttons and (for example) pre-stored images\nand text or an on-board, button-controlled user interface.\nTo complete the communication link between the badge and the user, the controlling node\nwas decided to be the user‚Äôs smartphone. Considering that the vast majority of likely users\nwill be teenage students or older, it is safe to assume that every potential user will own a\nsmartphone capable of connecting wirelessly to the badge. As smartphones pack processing\npower capable of much more demanding tasks, all tasks related to connection and control\nof the badge, as well as providing the user interface for text input and drawing, would be\noffloaded to the smartphone device.\nTherefore, the badge core functionality is limited to establishing a connection with the\nsmartphone, receiving data wirelessly and displaying the incoming image (which may show-\ncase text, drawings or other) to a display.\nOther functional requirements of the badge include an onboard battery to allow the product\nto operate as a standalone unit. A micro-controller is also included, in order to act as the\nreceiving node of the wireless connection and as an operator of the on-board display."},"University/Sonic-AR-navigation-for-cyclists":{"title":"AR navigation for cyclists","links":["assets/papers/Mobile_HCI_Coursework.pdf"],"tags":[],"content":"\nAugmented Reality for Cyclists: A New Navigation Experience üö¥‚Äç‚ôÇÔ∏è\nAs a cyclist, one is much more exposed than other modes of transport. For this reason, navigating busy streets while keeping an eye on the road can be dangerous challenge. Enter the world of Augmented Reality (AR) to improve the way cyclists navigate.\nThe Vision üåü\nOur team explored the application of AR navigation for cyclists. This application aims to provide both audio and visual cues to help cyclists navigate without diverting their attention from the road. Designed for smartphones and AR glasses, this app promises an intuitive and helpful experience.\nNote: The primary goal is to ensure that users can focus on the road while seamlessly receiving directions.\n\nHow Does It Work? ü§î\nThe application leverages the camera and GPS functionalities of smartphones. It overlays waypoints onto the real world, guiding the cyclist through their journey. Imagine having a virtual waypoint illuminated right in front of you, ensuring you never miss a turn and never need to look away from the road!\nAddressing Potential Distractions\nThe team is acutely aware of the potential distractions that visual and auditory feedback can cause. Hence, they‚Äôve chosen specific interaction concepts to mitigate these concerns:\n\n\nBinaural Peripheral Audio Cues: These are 3D sound cues that help in determining the correct direction by panning the audio between the left and right ear. The sound is a low-passed periodic wave, meaning its presence aims to minimize interference with the existing environmental sounds.\n\n\nAnimated Visual Waypoints: Rather than informing the user through occluding their field of view with text or by lighting up the entire path, the destination is visible as a waypoint which periodically fades in and out, allowing the full environment to be viewed.\n\n\nWhy This Solution?\nWe believed that the AR solution offered a unique and innovative approach to enhance the cycling experience. Not only does it promise to improve navigation, but it also aimed to elevate the safety of cyclists on the road.\nIn conclusion, we believe the future of cycling navigation could look promising with the integration of AR technology and encourage anyone to draw inspiration from our investigation. Please feel free to read the paper for more information."},"index":{"title":"üåê Welcome to a corner of the internet","links":["Side-projects/Compass","Side-projects/Radiooooo-download-button","Home-Assistant/Kokoro-wyoming-text-to-speech","University/Genre-classification","University/ChatGPT-Music-Player-plugin","University/Open-day-badge","University/Sonic-AR-navigation-for-cyclists"],"tags":[],"content":"Breathe.. have a glass of water.. and take a look around, see if there‚Äôs anything of interest\nThere‚Äôs not much here at the moment, but I‚Äôm always working on something in my spare time and I plan to write freely about it for those curious enough to read.\nHave a look to the left üëàüèΩ to get an overview of my posts\nAbout me\nWith a background in electronics and software, I love building and exploring new technologies and this is why I‚Äôm a huge fan of open source projects - they allow me to do just that.\nHacking the site üòé\nDid you know that every website you visit is technically open source?\nTo start with, try updating the CSS below from color black to red:\n\np {\n    color: black;\n}\n\nYou can view and edit the code of any website by right clicking and selecting ‚ÄúInspect‚Äù!\nHere‚Äôs a fun tutorial to learn how to edit the colors of any website: Click here to go to the tutorial\nA few of my favourite things\nMy favourite thing is to look at the trending projects on GitHub, or at the top models on Hugging Face, and see what people are building. Then, if something catches my interest, I‚Äôll clone the repo and run and explore it. It‚Äôs free and open source, so you can do the same!\nLately, I‚Äôve been getting more more interested in contributing to this great community, and started to build a few projects that I‚Äôll be writing about here..\nSo far I‚Äôve added a few posts about some of my side projects and stuff I did at uni - hover over to get a preview!\nSide projects\n\nCompass\nRadiooooo download button\nKokoro wyoming text-to-speech\n\nAI\n\nGenre classification\nChatGPT Music Player plugin\n\nHardware\n\nOpen day badge\n\nAugmented Reality\n\nSonic AR-navigation for cyclists\n"}}